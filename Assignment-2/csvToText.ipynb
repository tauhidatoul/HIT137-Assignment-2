{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Convert CSV to TXT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def csv_to_text(csv_filename, text_filename):\n",
    "    with open(csv_filename, 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        data = list(csv_reader)\n",
    "\n",
    "    with open(text_filename, 'w') as text_file:\n",
    "        for row in data:\n",
    "            text_file.write('\\t'.join(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_text('CSV1.csv', 'output1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_text('CSV2.csv', 'output2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_text('CSV3.csv', 'output3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_text('CSV4.csv', 'output4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to remove numbers from the txt file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_numbers(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as input_file:\n",
    "        content = input_file.read()\n",
    "\n",
    "    # Use regular expression to remove numbers\n",
    "    content_without_numbers = re.sub(r'\\d+', '', content)\n",
    "\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        output_file.write(content_without_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_numbers('output1.txt','outputWithOutNum1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_numbers('output2.txt','outputWithOutNum2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_numbers('output3.txt','outputWithOutNum3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_numbers('output4.txt','outputWithOutNum4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as input_file:\n",
    "        content = input_file.read()\n",
    "\n",
    "    # Remove all punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    content_without_punctuation = content.translate(translator)\n",
    "\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        output_file.write(content_without_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation('outputWithOutNum1.txt','removedPunc1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation('outputWithOutNum2.txt','removedPunc2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation('outputWithOutNum3.txt','removedPunc3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation('outputWithOutNum4.txt','removedPunc4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLTK to Remove Stop Words. To eliminate words like “the”, “a”, “an”, or “in”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\barua\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as input_file:\n",
    "        content = input_file.read()\n",
    "\n",
    "    # Tokenize the content into words\n",
    "    words = nltk.word_tokenize(content)\n",
    "\n",
    "    # Remove English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Join the filtered words back into a string\n",
    "    content_without_stopwords = ' '.join(filtered_words)\n",
    "\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        output_file.write(content_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords('removedPunc1.txt', 'removedUnnecessary1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords('removedPunc2.txt', 'removedUnnecessary2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords('removedPunc3.txt', 'removedUnnecessary3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords('removedPunc4.txt', 'removedUnnecessary4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Remove Unnecessary Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary_spaces(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as input_file:\n",
    "        content = input_file.read()\n",
    "\n",
    "    # Remove unnecessary spaces\n",
    "    content_without_unnecessary_spaces = re.sub(r'\\s+', ' ', content)\n",
    "\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        output_file.write(content_without_unnecessary_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_unnecessary_spaces('removedUnnecessary1.txt', 'removedSpaces1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_unnecessary_spaces('removedUnnecessary2.txt', 'removedSpaces2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_unnecessary_spaces('removedUnnecessary3.txt', 'removedSpaces3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_unnecessary_spaces('removedUnnecessary4.txt', 'removedSpaces4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Remove single character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_character_words(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as input_file:\n",
    "        content = input_file.read()\n",
    "\n",
    "    # Remove all single-character words\n",
    "    content_without_single_character_words = re.sub(r'\\b\\w\\b', '', content)\n",
    "\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        output_file.write(content_without_single_character_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_single_character_words('removedSpaces1.txt', 'removedSingleWords1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_single_character_words('removedSpaces2.txt', 'removedSingleWords2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_single_character_words('removedSpaces3.txt', 'removedSingleWords3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_single_character_words('removedSpaces4.txt', 'removedSingleWords4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting File Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with the pattern 'removedSingleWords' in the folder:\n",
      "removedSingleWords1.txt\n",
      "removedSingleWords2.txt\n",
      "removedSingleWords3.txt\n",
      "removedSingleWords4.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_files_with_pattern(folder_path, pattern):\n",
    "    matching_files = [file for file in os.listdir(folder_path) if pattern in file]\n",
    "    return matching_files\n",
    "\n",
    "# Replace 'your_folder_path' with the actual path to your folder\n",
    "folder_path = 'C:/Users/barua/CDU_Assignment2_workingWithCSV'\n",
    "pattern_to_match = 'removedSingleWords'\n",
    "\n",
    "matching_files = get_files_with_pattern(folder_path, pattern_to_match)\n",
    "\n",
    "print(f\"Files with the pattern '{pattern_to_match}' in the folder:\")\n",
    "for file in matching_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['removedSingleWords1.txt',\n",
       " 'removedSingleWords2.txt',\n",
       " 'removedSingleWords3.txt',\n",
       " 'removedSingleWords4.txt']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to add all txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_files(input_filenames, output_filename):\n",
    "    with open(output_filename, 'w') as output_file:\n",
    "        for input_filename in input_filenames:\n",
    "            with open(input_filename, 'r') as input_file:\n",
    "                content = input_file.read()\n",
    "                output_file.write(content + '\\n')  # Add a newline between files\n",
    "\n",
    "# Replace the list of 'input_file1.txt', 'input_file2.txt', etc., with your actual file names\n",
    "input_files = matching_files\n",
    "output_file = 'output_file.txt'\n",
    "\n",
    "concatenate_files(input_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_unnecessary_spaces('output_file.txt', 'removedSpacesOutPutFile.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Count top 30 Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 most common words:\n",
      "mg: 916142\n",
      "name: 719508\n",
      "patient: 650221\n",
      "blood: 635809\n",
      "po: 591152\n",
      "tablet: 560514\n",
      "discharge: 487365\n",
      "hospital: 466831\n",
      "daily: 440213\n",
      "day: 416519\n",
      "left: 404974\n",
      "sig: 382216\n",
      "one: 376046\n",
      "history: 357927\n",
      "last: 352837\n",
      "pm: 333899\n",
      "right: 328977\n",
      "admission: 284736\n",
      "pain: 274064\n",
      "pt: 258561\n",
      "normal: 256153\n",
      "ct: 243239\n",
      "date: 215380\n",
      "namepattern: 194330\n",
      "medications: 188840\n",
      "chest: 185455\n",
      "first: 180360\n",
      "given: 167781\n",
      "also: 158447\n",
      "course: 155930\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_top_words(filename, top_n=30):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the text into words using regular expression\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Convert to lowercase for case-insensitive counting\n",
    "\n",
    "    # Count occurrences of each word\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Get the top N most common words\n",
    "    top_words = word_counts.most_common(top_n)\n",
    "\n",
    "    return top_words\n",
    "\n",
    "# Replace 'your_text_file.txt' with the actual path to your text file\n",
    "filename = 'removedSpacesOutPutFile.txt'\n",
    "top_words = get_top_words(filename)\n",
    "\n",
    "print(\"Top 30 most common words:\")\n",
    "for word, count in top_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to add top 30 common words in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 most common words and counts have been written to 'output.csv'.\n"
     ]
    }
   ],
   "source": [
    "def get_top_words(filename, top_n=30):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the text into words using regular expression\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  # Convert to lowercase for case-insensitive counting\n",
    "\n",
    "    # Count occurrences of each word\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Get the top N most common words\n",
    "    top_words = word_counts.most_common(top_n)\n",
    "\n",
    "    return top_words\n",
    "\n",
    "def write_to_csv(output_filename, top_words):\n",
    "    with open(output_filename, 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['Word', 'Count'])  # Write header\n",
    "\n",
    "        for word, count in top_words:\n",
    "            csv_writer.writerow([word, count])\n",
    "\n",
    "# Replace 'your_text_file.txt' and 'output.csv' with the actual file names\n",
    "filename = 'removedSpacesOutPutFile.txt'\n",
    "output_csv_filename = 'output.csv'\n",
    "\n",
    "top_words = get_top_words(filename)\n",
    "write_to_csv(output_csv_filename, top_words)\n",
    "\n",
    "print(f\"Top 30 most common words and counts have been written to '{output_csv_filename}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
